{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \"Data mining for hidden gems in data\"\n",
    "\n",
    "> \" My take on knowledge discovery from datasets\"\n",
    "\n",
    "- toc: true \n",
    "- comments: true\n",
    "- categories: [machine learning, jupyter notebooks, data science, project workflow, data mining, crisp dm]\n",
    "- image: images/8.png\n",
    "- permalink: /datascience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is data mining\n",
    "\n",
    "This is my take on responsibilities, demands, processes and professionalism for having a robust problem solving arsenal when dealing with data. \n",
    "\n",
    "After working extensively in financial markets for the last 5 years, I know the delight and power of having great insight. It saves you time and dramatically changes your trajectory for success. \n",
    "\n",
    "I see 2 versions of data mining. One based on a process of data processing that is well researched and documented. This involves answering questions have been solved- and their variations, its a matter of knowing the models that apply to your situation. Think recommendation engines, or a spam comment filters, the process is known. The nuances are getting it work for you, and finetunning for deployment.  \n",
    "\n",
    "In the other, has general business questions like, 'How can a logistics company increase revenue by taking on more diverse kinds of deliveries? This problem has many moving parts. \n",
    "It needs statistical research, domain expertise, and creativity. To get good at this kinds of problems, you need data journalism, information design, data simulation and a good handle of programming and algorithm design. \n",
    "\n",
    "\n",
    "The obvious questions are, 'How can it go wrong?' and 'How can be the one to see it before it happens?'. Consicely:\n",
    "\n",
    "- Do I have data or do have to simulate it?\n",
    "- Is the data represented correctly?\n",
    "- Is data integrity maintained in all stages of during reading or writing?\n",
    "- What are the right, ambitious and wrong questions to ask? \n",
    "- Is our automation and coding skill up to par? \n",
    "- Do I have a strong defensible foundation for choice of tools, algorithms and design choices?\n",
    "- How will I communicate and deliver findings?\n",
    "- What should the ouput look?\n",
    "- What I'm I looking for, or atleast what Im I looking at?\n",
    "- How do I decouple the solution into simple abstractions that I can think about?\n",
    "\n",
    "\n",
    "As the name sugguests data mining is knowledge discovery from datasets. \n",
    "\n",
    "Its searching through data to come up with valuable information that can be used as new knowledge for existing problems. In searching, data representation maybe you most important goal. Some creativity does come to mind, but there are generally acceptable ways. \n",
    "\n",
    "Problems are reduced to quantifiable sums as hypothesis statements.The data mining process is structured and relies on good statistical reasoning and explainability; for this reason, the solution needs to be generalizable and reproducable. \n",
    "\n",
    "The output of data mining models is simple, robust and translatable/interpratable for it to be plugged into the value chain, eg... the similarity score of netflix movies is an easily interpratable number- 0-100. Easily understandable at a glance though the underlying model maybe complex. \n",
    "\n",
    "The data mining strategy uses a data driven approach on a vast number of business problems and opportunities,\n",
    "with Data Scietists working on huge number of seemingly unrelated problems; ranging from \n",
    "social impact, medical imaging, law, politics and hobby projects. \n",
    "\n",
    "This hints that there is strong underlying workflow for success in data mining. \n",
    "\n",
    "Data Science as a role, makes data and skills available for data driven approaches to strategic issues. \n",
    "\n",
    "Data strategies vary in complexity; from a simple google form questionnaire asking for feedback to specialised consultation services. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The  data mining project workflow\n",
    "\n",
    "No data mining intro would be complete without the CRISP-DM model...\n",
    "\n",
    "Below is a variant of the CRISP-DM model. \n",
    "\n",
    "1. Business opportunity and hypothesis formulation \n",
    "\n",
    "The first part is envision the business oportunity, then understanding the problem domain, questioning assumptions and reading widely and scantily for possible solutions. You would then formulate hypothesis that best seek to enlighten opinions. You would also check the quality and quantity of data and how it is served. Record any inconsistencies in data, and assupmtions that may be important further down the line. Loosely couple your moving pieces and create an abstraction layer. \n",
    "\n",
    "  - Business opportunity\n",
    "  - Find data sources. Availability and Suitabiity\n",
    "  - Explore and visualize data. Predictions and Insights\n",
    "  - Ascertain consistency or account for inconsistencies\n",
    "  - Decouple the pieces of the solution and design your abstraction layer\n",
    "\n",
    "\n",
    "\n",
    "2. Proposing a model solution. Its implementation and serving  \n",
    "\n",
    "With credible exploratory data analysis and domain research, you have an idea of possible models that should have decent results. Once you have some base models, you can tweek them with more data or engineer features. Engineering features is going to be the most rewarding addition to your model accuracy, spend some time on it.\n",
    "\n",
    "  - Clean data \n",
    "  - Feature engineer data\n",
    "  - Add data\n",
    "  - Train models \n",
    "  - Model ensemble\n",
    "  - Audit best model\n",
    "  - Model explainability \n",
    "  \n",
    "\n",
    "\n",
    "3. Auditing model for solution implementation \n",
    "\n",
    "To ascertain your model resilience on production data, you must constantly audit data sources, inputs, pipelines and data warehouses and keep tabs on any changes that affect your model accuracy. Further to checking data being fed into the model, also use best software practices for data mining. This include:\n",
    "\n",
    "  - Unit testing modules and functions and all abstration layers \n",
    "  - Using model pipelines \n",
    "  - Logging critical parts of your softare layer\n",
    "  - Proper documentation and defensible design and model choices\n",
    "  - Accuracy scoring and model decay checking\n",
    "  - Retraining, updating and retiring models\n",
    "  \n",
    "  \n",
    "Steps 2 and 3 are iterative. Thats is why its is important to create an asbraction layer and loose coupling early on. Your whole project shouldnt crush on every iteration. You can work on one part without thinking much about the other parts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "\n",
    "Not all data is the same. Some data is timely, has more volume, better quality, verifiable, better formated and domain relevant to your situation. \n",
    "\n",
    "Data needs to be organized from many formats, departments and time scales and made available on the data platform/lake. This data availability creates relevance to data, since it can now be use as a whole and efficiently integrated in other systems. Data warehouses can take various shapes and sizes, depending on the nature, format and usecase. Some can have data of the same format. While others can mix various data formats and sources. \n",
    "\n",
    "Data might also be missing. You might have to buy it, engineer it or account for its absence in your projects.\n",
    "\n",
    "There needs to be good data engineering practices for the most basic and widely applicable data needs and tools. i.e data backup, databases querry, big data tools, data pipelines and parallel computing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip:  [Learn SQL from DataCamp](https://www.datacamp.com?tap_a=5644-dce66f&tap_s=1363842-0cfaf5&utm_medium=affiliate&utm_source=lawrencenderitu&tm_subid1=indexpage). The most in demand database infrastructure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data scientist \n",
    "\n",
    "![](../images/d_scientist.jpg)\n",
    "\n",
    "A data scientist is a manager, a designer and the stake holder of the processes above. \n",
    "\n",
    "The data scientist enumerates project value, potential constraints, moving parts and reservations about projects. \n",
    "\n",
    "Data scientists monitor modifications and updates to business opportunities, implementation constraints, management constraints, regulatory constraints, data constraints, skill contraints and tooling constraints.\n",
    "\n",
    "They keep a score card of past analytics projects, approaches applied and any changes in views to current data driven approaches in the business. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like to think of data scientist as a great information designer, who is able to have great perspecives on the data he is working with. He is able to represent it in a way that the most important information is simple to discern and utilise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit735b8921aa184eefb4e65f4f9da620bb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
