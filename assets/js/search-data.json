{
  
    
        "post0": {
            "title": "Data Science- What is it?",
            "content": "What is data science . This may look like a simple question to answer but the more you go over the internet, the more you realise the many number of subfields in this topic. Luckily for us, the word &#39;science&#39; in &#39;data science&#39; generalizes the definition for us. Data science is thus the entire genre of skills and processes and art, used in finding meaning in data. The subfields in data science are due to the distintions, ifluences and applications in methods used to arrive at the solution. Major distinctions are, statistical inference, machinelearning, deeplearning. These too have their own subcategories, depicting specific methods used. . Machine learning seems to be the operative term when talking about data science, and rightly so. Most advancements in data science that have given it its new hype, stem from machine learning. Machine learning has been able to solve some peculiar set of problems that were previously out of reach with traditional methods. Most famous fields are,computer vision, gameplay, natural language and autonomous systems. It hasnt stopped there, machine learning`s ripple effects and cross advantages are disrupting whole industries. New companies are in a short period of time able to take over long standing industries. Ignoring the hype is beginning to look like a fools errand, with every industry having a machine learning powered product. . Machine learning uses using data to learn. Instead of hardcoding rules, the machine learning algorithms make their inference from previous observations and feedack, just like humans learn to play a game. . Ideological purpose behind data science . To the ideologist, the purpose behind data science it to formally account for the process of knowledge discovery from data. This becomes a formal process of creating hypotheses, the language, grammar, semantics and constraints for that field. This goes further into automated decision making and reproducability. Thus, specifying as much of the process as closely and accurately as possible. Detailing the steps towards recreacting the scenarios for specialised systems. . Before a model is chosen, there is a formal data analysis phase. Forecasting, descriptive, predictive and prescriptive analysis, to vouche that the data fits into the formal constraints for a chosen model to work. . To the ideologist, all he wants it to detect patterns, create new patterns or foretell them with as much accuracy and understanding as possible. So he divices ways to think about his problems. At first, the methods are simple averages, minimums, maximums, modes etc. But, this dont suffice, so he goes further, and creates specialised methods like trees, random forrest, KNNs etc. He creates rules for when to use them and when not to. He creates a whole language for explaining, his problem, approaches, reasoning and the solution design. Each kneatley organized for the next iteration and improvement. . He further postulates that since, he is learning from data and not hardcoding rules, neural networks would work best. . Data science thus becomes a discipline to advance and formalise the language, techniques and tasks of knowledge discovery. . Finance, Politics, Marketing, Ethics, Society and Ethics . Machine learning, and the explosion of big data has been coming since the later 1870s with the invention of the first telephone. We can follow that line of communication down to the radio, the tv, the computer, the smart phone, and now the smart devices like siri and amazon echo. Just like space travels owes its progress to the steam engine. . This line brought along its parallels, influences and ripple effects. Most observable in how it influenced, society, computing, information, business and profits. Among the most notable are the connected information, the networking of individuals, the creation of new information by computers, and the recording of information into digital formats. In each of these parallels, there promises to be business, profit and power to be had from studying and better undertanding, and advancing it. . Now, we can tell when an individual watches an ad, makes a purchase, recommends its, is home for item delivery. We can even tell political views, how to ifluence them and tell excatly when someones political opinion has changed. So the job of these social sciences becomes to improve and understand the effect of this new reality of living with data, how it can be used for good and also for bad. . Data science is hence used to shape social policies that govern how much data can be collected, who get to collects and monetize. Most importantly, its informing the individual on his rights, and posibilities of data breaches from the devices they use. . Data privacy and data ownership. . Machine learning and data science as a neccessity . With the size, complexity and variability of new data, we need new methods to process it and analyse it, otherwise, it will suffocate us under its weight. Every individual today with a smartphone has more computing power that the apollo mission. This new demands have heraled a shift in computer achitectures and a demand for new skills ripe for this new computer age. . Think about it, every new company has its new way of doing business be it, tracking orders, paying for parking tickets, delivering content, taking pictures, handling events, the list goes on. Machine learning, data science and artificial intelligence is the natural step of a long of advancements in engineering. . Deeplearning and AI . Deelearning is what buzzing.. . Deeplearning is at the edge of AI. AIs goal is to create human intelligence in machines. the end goal is to create general ai. this type of ai can learn to solve alot of problems. like reading, writing. currently, ai systems are specialised. . summary, parts, i t . Common data science algorithms for common patterns in data . From the ideologies above, there emerges common algorithms that work very well on a certain data sets or certain problems. supervised, unsupervised and reinforcement learning . linear reg, . categorical, . knn . decision trees . random forest . svd . logistic regression . supervised . unsupervised . Applications of data science in the wild . one of the best examples of the wide application of machine learning is in kaggle competitions. . agriculuture weather computer vision . Data science for the individual or small company . you may not have enough data for the larger task, but dont let that hold you back. some models are pretrained and fitted for specialised tasks like detecting faces, cats, dogs etc you can play around with these . Self serve data science as an API . data science is still a software endgoal. that means that the processing is served on an api and served to the end use. below is model that can . The data science process workflow . No data science intro would be complete without the CRISP-DM model... . Below is my variant of the CRISP-DM model. . 1. Business opportunity and hypothesis formulation . The first part is envision the business oportunity, then understanding the problem domain, questioning assumptions and reading widely and scantily for possible solutions. You would then formulate hypothesis that best seek to enlighten opinions. You would also check the quality and quantity of data and how it is served. Record any inconsistencies in data, and assupmtions that may be important further down the line. Loosely couple your moving pieces and create an abstraction layer. . Business opportunity | Find data sources. Availability and Suitabiity | Explore and visualize data. Predictions and Insights | Ascertain consistency or account for inconsistencies | Decouple the pieces of the solution and design your abstraction layer | . 2. Proposing a model solution. Its implementation and serving . With credible exploratory data analysis and domain research, you have an idea of possible models that should have decent results. Once you have some base models, you can tweek them with more data or engineer features. Engineering features is going to be the most rewarding addition to your model accuracy, spend some time on it. . Clean data | Feature engineer data | Add data | Train models | Model ensemble | Audit best model | Model explainability | . 3. Auditing model for solution implementation . To ascertain your model resilience on production data, you must constantly audit data sources, inputs, pipelines and data warehouses and keep tabs on any changes that affect your model accuracy. Further to checking data being fed into the model, also use best software practices for data mining. This include: . Unit testing modules and functions and all abstration layers | Using model pipelines | Logging critical parts of your softare layer | Proper documentation and defensible design and model choices | Accuracy scoring and model decay checking | Retraining, updating and retiring models | . Steps 2 and 3 are iterative. Thats is why its is important to create an asbraction layer and loose coupling early on. Your whole project shouldnt crush on every iteration. You can work on one part without thinking much about the other parts. . . The data science domain language. . for us to understand if clearly, we must understand the domain language of data science. With each subdomain, lies another language domain. The sooner we understand the terms, meanings, processes inside the terms, assumptions, influences, it becomes less foreign, the sooner we can speak it. And when we speak it, we become use it and if we can use it, we cna grow it. . This is my take on responsibilities, demands, processes and professionalism for having a robust problem solving arsenal when dealing with data. . I see 2 versions of data mining. One based on a process of data processing that is well researched and documented. This involves answering questions have been solved- and their variations, its a matter of knowing the models that apply to your situation. Think recommendation engines, or a spam comment filters, the process is known. The nuances are getting it work for you, and finetunning for deployment. . In the other, has general business questions like, &#39;How can a logistics company increase revenue by taking on more diverse kinds of deliveries? This problem has many moving parts. It needs statistical research, domain expertise, and creativity. To get good at this kinds of problems, you need data journalism, information design, data simulation and a good handle of programming and algorithm design. . The obvious questions are, &#39;How can it go wrong?&#39; and &#39;How can be the one to see it before it happens?&#39;. Consicely: . Do I have data or do have to simulate it? | Is the data represented correctly? | Is data integrity maintained in all stages of during reading or writing? | What are the right, ambitious and wrong questions to ask? | Is our automation and coding skill up to par? | Do I have a strong defensible foundation for choice of tools, algorithms and design choices? | How will I communicate and deliver findings? | What should the ouput look? | What I&#39;m I looking for, or atleast what Im I looking at? | How do I decouple the solution into simple abstractions that I can think about? | . As the name sugguests data mining is knowledge discovery from datasets. . Its searching through data to come up with valuable information that can be used as new knowledge for existing problems. In searching, data representation maybe you most important goal. Some creativity does come to mind, but there are generally acceptable ways. . Problems are reduced to quantifiable sums as hypothesis statements.The data mining process is structured and relies on good statistical reasoning and explainability; for this reason, the solution needs to be generalizable and reproducable. . The output of data mining models is simple, robust and translatable/interpratable for it to be plugged into the value chain, eg... the similarity score of netflix movies is an easily interpratable number- 0-100. Easily understandable at a glance though the underlying model maybe complex. . The data mining strategy uses a data driven approach on a vast number of business problems and opportunities, with Data Scietists working on huge number of seemingly unrelated problems; ranging from social impact, medical imaging, law, politics and hobby projects. . This hints that there is strong underlying workflow for success in data mining. . Data Science as a role, makes data and skills available for data driven approaches to strategic issues. . Data strategies vary in complexity; from a simple google form questionnaire asking for feedback to specialised consultation services. . the algos, language, constraints. . product. utility, motivation and potential . The data . Not all data is the same. Some data is timely, has more volume, better quality, verifiable, better formated and domain relevant to your situation. . Data needs to be organized from many formats, departments and time scales and made available on the data platform/lake. This data availability creates relevance to data, since it can now be use as a whole and efficiently integrated in other systems. Data warehouses can take various shapes and sizes, depending on the nature, format and usecase. Some can have data of the same format. While others can mix various data formats and sources. . Data might also be missing. You might have to buy it, engineer it or account for its absence in your projects. . There needs to be good data engineering practices for the most basic and widely applicable data needs and tools. i.e data backup, databases querry, big data tools, data pipelines and parallel computing . . Tip: Learn SQL from DataCamp. The most in demand database infrastructure. . The data scientist . A data scientist is a manager, a designer and the stake holder of the processes above. . The data scientist enumerates project value, potential constraints, moving parts and reservations about projects. . Data scientists monitor modifications and updates to business opportunities, implementation constraints, management constraints, regulatory constraints, data constraints, skill contraints and tooling constraints. . They keep a score card of past analytics projects, approaches applied and any changes in views to current data driven approaches in the business. . I like to think of data scientist as a great information designer, who is able to have great perspecives on the data he is working with. He is able to represent it in a way that the most important information is simple to discern and utilise. .",
            "url": "https://maclawry.github.io/fastblog/datascience",
            "relUrl": "/datascience",
            "date": " • Jan 8, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Hello flower, a data mining approach",
            "content": ". Brief Intro . Its good to simplify a problem and walk over it till the end. We choose the cleanest most agreeable data to walk through our data science processs. But first, we explain what machine learning is, and how supervised and unsupervised models differ. . Supervised and Unsupervised Learning . Why machine learning? Machine learning helps solve a unique class of problems. Take for example, facial recognition, or language translation. These problems come ever close to how humans perceive the world. Such that machines are now an embeded part of human interaction, without which, we feel less of ourselves...This is only the beginning. . Machine learning has 3 main flavours, Supervised, Unsupervised and Reinforcement Learning. The main data structures are tabular data, image data, language and timeseries. In supervised learning, we know what the ground truth is. We have recorded enough outcomes given certain events and interactions. The outcomes are the labels or dependent variables (y), that are the end product of feature interactions of independent variables(x). A model is a recipe of features that can map feature interaction to a label with certain degree of acceptance. The degree of acceptance is accuracy in a reproducable and generalised way. Simply put, a model is a function that maps x(s) to y. . In unsupervised learning, we dont have labels, we learn labels or categories from features. . More about simple supervised models. . Supervised models are either classification or regression type. Classification models predict the class labels from a finite group i.e boy or girl, species of plant, type of disease etc. Regression models predict labels as continous variables. i.e house price, fuel consumption etc. . The enterprise workflow . This workflow is explained futher here. . Step 0 Problem statement and libraries needed . We are given an array of petal and sepal measurements of 3 iris flower species. Our role is predict the species correctly if given new taxonomic measurements of petals and sepals of the same species of flowers. This is clearly a classification problem. . # the data is already in sklearn. from sklearn import datasets import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns plt.style.use(&#39;ggplot&#39;) . Step 1: Loading and preliminary data inspection . iris = datasets.load_iris() print(f&#39;type iris: {type(iris)}&#39;) print(f&#39;iris keys: {iris.keys()}&#39;) print(f&#39;type iris.data: {type(iris.data)}&#39;) print(f&#39;type iris.target: {type(iris.target)}&#39;) . type iris: &lt;class &#39;sklearn.utils.Bunch&#39;&gt; iris keys: dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;]) type iris.data: &lt;class &#39;numpy.ndarray&#39;&gt; type iris.target: &lt;class &#39;numpy.ndarray&#39;&gt; . . Note: this is a dictionary of numpy.array values. We will have to create out pandas dataframe using the keys. . df= pd.DataFrame(iris.data, columns= iris.feature_names) df.info() df.describe() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 150 entries, 0 to 149 Data columns (total 4 columns): # Column Non-Null Count Dtype -- -- 0 sepal length (cm) 150 non-null float64 1 sepal width (cm) 150 non-null float64 2 petal length (cm) 150 non-null float64 3 petal width (cm) 150 non-null float64 dtypes: float64(4) memory usage: 4.8 KB . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . count 150.000000 | 150.000000 | 150.000000 | 150.000000 | . mean 5.843333 | 3.057333 | 3.758000 | 1.199333 | . std 0.828066 | 0.435866 | 1.765298 | 0.762238 | . min 4.300000 | 2.000000 | 1.000000 | 0.100000 | . 25% 5.100000 | 2.800000 | 1.600000 | 0.300000 | . 50% 5.800000 | 3.000000 | 4.350000 | 1.300000 | . 75% 6.400000 | 3.300000 | 5.100000 | 1.800000 | . max 7.900000 | 4.400000 | 6.900000 | 2.500000 | . df.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . Step 2 Data Exploration and Visualization . We always want to visualize data. The reason is 2 fold. It will help you draw up conclusions fast and it will most likely be the method in which you communicate your findings. . sns.heatmap(df.corr()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f42f8a4a7d0&gt; . sns.pairplot(df) . &lt;seaborn.axisgrid.PairGrid at 0x7f42f8aa43d0&gt; . TODO: Add notes We aready know that our label had class of 3 flower . Step 3: Training an appropriate base model . from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors = 6) x , y = iris.data, iris.target knn.fit(x, y) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=6, p=2, weights=&#39;uniform&#39;) . . Note: note we did not fit on the dataframe. We fit our model on nympy.array. . x.shape, y.shape, type(x), type(y) . ((150, 4), (150,), numpy.ndarray, numpy.ndarray) . Step 4 Making predictions on test data . new_data = np.array([[5.6, 2.8, 3.9, 1.1], [4.0, 2.1, 1.0, 0.2], [4.3, 3.6, 1.0, 0.3], [5.7, 2.6, 3.8, 1.3]]) prediction = knn.predict(new_data) print( prediction) . [1 0 0 1] . Step 5 Perfomance Metrics . What model would be compelete if did not try to measure how well it perfomed. Say, a scientist handed us new data, kneatly formated to suit our training data (how thoughtful), how convincing is our classification model in accurately labeling the dataset? . Model accuracy. It means that your model is verifiable, generalizable and reproducable. . Our model, has no more data to test on, we used all our data to train. This is obviously a problem. We cant pose the same questions to our intelligent model, that we used to train it on. A 100% accuracy wouldnt be imperessive in this scenario. . So what should we have different? . Splitting our data into train, test and validation set. . For now, we stick to train and test. . from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score xtrain, xtest, ytrain, ytest = train_test_split(x, y , test_size = 0.2, random_state = 1, stratify = y) knn= KNeighborsClassifier(n_neighbors=8) knn.fit(xtrain, ytrain) ypred = knn.predict(xtest) print(ypred) . [2 0 1 0 0 0 2 2 2 1 0 1 2 1 2 0 2 1 1 2 1 1 0 0 2 1 0 0 1 1] . print(f&#39;score: {knn.score(xtest, ytest)}&#39;) print(f&#39;accuracy: {accuracy_score(ypred, ytest)}&#39;) . score: 0.9666666666666667 accuracy: 0.9666666666666667 . . Note: knn.score calls accuracy_score under the hood, that is why they give the same result. . We are at 97% model accuracy in telling apart iris flower species. . It be nice if we say the predicions as flower species names as opposed to numbers? Lets decode the predictions . from sklearn.preprocessing import LabelEncoder le = LabelEncoder().fit(iris.target_names) le.inverse_transform(ypred) . array([&#39;virginica&#39;, &#39;setosa&#39;, &#39;versicolor&#39;, &#39;setosa&#39;, &#39;setosa&#39;, &#39;setosa&#39;, &#39;virginica&#39;, &#39;virginica&#39;, &#39;virginica&#39;, &#39;versicolor&#39;, &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;, &#39;versicolor&#39;, &#39;virginica&#39;, &#39;setosa&#39;, &#39;virginica&#39;, &#39;versicolor&#39;, &#39;versicolor&#39;, &#39;virginica&#39;, &#39;versicolor&#39;, &#39;versicolor&#39;, &#39;setosa&#39;, &#39;setosa&#39;, &#39;virginica&#39;, &#39;versicolor&#39;, &#39;setosa&#39;, &#39;setosa&#39;, &#39;versicolor&#39;, &#39;versicolor&#39;], dtype=&#39;&lt;U10&#39;) . We now have a canvas to start with, not one we are proud of, but a canvas no matter. We have over simplified our responsibilites, on purpose. Later, we will revisit each step, grow the number of steps and try more demanding challenges. . To Learn more click ) .",
            "url": "https://maclawry.github.io/fastblog/mlworkflow",
            "relUrl": "/mlworkflow",
            "date": " • Jan 19, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This is my rendition of the IBM Analytics Data Science Competency Model through projects and notes. It is in no partcicular order and I will most likely not represent the core deliverables of the blue print, but I will try. . You can reach me on . TODO: add contact form .",
          "url": "https://maclawry.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Consult",
          "content": "TODO: add business ready projects here… .",
          "url": "https://maclawry.github.io/fastblog/consult/",
          "relUrl": "/consult/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://maclawry.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}