{
  
    
        "post0": {
            "title": " What is Data Science",
            "content": "What is Data Science . Data science is project based solving of strategically important problems. These problems are reduced to quantifiable sums to avoid generalities in hypothesis statement formulation. . A hospital might ask, how can we reduce the number of re-admitions, a sales manager mights ask, how can I make more sales from my current team and leads. From here a data scientist will ask leading questions adn come up with a hypothesis after understanding your business domain. eg. He might hypothesise, adding 3 more nurses per doctor reduces readmissions by 20%. . The data science strategy uses a data driven approach on a vast number of business problems and opportunities. It is the data scientists role to come up with data driven strategies with for the biggest opportunities within the organization. . Data Science therefore is the role that makes data available and the data skills to address strategic business issues. These strategies vary; from a simple google form questionnaire asking for feedback to specialised consultation services. . Data science solutions have inputs, and outcomes. The data scientists value is augmenting this relationship for desirable outcomes. . Data science project workflow . The list below outlines and agreeable data science workflow adaopted from the IBM Data Science Professional Competency Model. . Business opportunity and hypothesis formulation | The first part is understanding the problem domain, questioning assumptions and mostly reading widely and scantily for possible solutions. You would then formulate hypothesis that best seek to enlighten opinions. You would also check the quality and quantity of data and how it is served. Record any inconsistencies in data, and assupmtions that may be important further down the line. . Business opportunity | Find data sources. Availability and Suitabiity | Explore and visualize data. Predictions and Insights | Ascertain consistency or account for inconsistencies | . Proposing a model solution. Its delivery and serving | With credible data analysis reports, you can have a idea of possible models that should have decent results. Once you have some base models, you can tweek them with more data or engineer features. . Clean data | Feature engineer data | Additional data | Train models | Model ensemble | Deploy best model | Model explainability | . Auditing model and solution implementation | To ascertain your model works well on production data, you must constantly audit data sources, inputs, pipelines and data warehouses and keep tabs on any changes that affect your model accuracy. . Monitor and manage | Measure success | Model pipelines | Big data | Retrain or retire model | . Data . Not all data is equal. Some data is timely, better quality, verifiable, better formated and domain relevant to your situation. . Data needs to be organized from many formats, departments and time scales and made available on the data platform/lake. This organization creates relevance to data, since it can now be seen as a whole and efficiently integrated in other systems. . Data might also be missing. You might have to buy it, engineer it or account for its absence in you projects. . Analytics tools and methodologies . The choice of technology and design choice is a whole ecosystem that needs frquent, carefull monitoring. It will ultimately house your models. The accuracy, completion rates, cost and joy of working on a project will be heavily influenced by your analytics tools and methods. . choice of analytics tools and competing alternatives. Mostly influenced by industry trends, costs, culture and ease of integration into larger ecosystem. | benchmarks for quality, acurracy and speed. This will mostly be determined on a projects basis as a deliverable. | training on tools, configuration, maitanance and access. These standards are set by the data science manager adviced by business continuity models. | utilisation of tools per project and model. Measures the efficacy and utilisation of available tools. | IT infrasctructure. Your data tools sit on top or are fed into the larger IT infrascture. | models shipped and design choices used. Score of success that advices future tech, hires and design choices. | maintainance audits. Frequent of servicing models and refactoring of code. | . Data scientist . A data scientist is a manager, a designer and the stake holder of the processes above. . The data scientist enumerates project value, potential constraints, moving parts and reservations about projects. . Data scientists monitor modifications and updates to business opportunities, implementation constraints, management constraints, regulatory constraints, data constraints, skill contraints and tooling constraints. . They keep a score card of past analytics projects, approaches applied and any changes in views to current data driven approaches in the business. .",
            "url": "https://maclawry.github.io/fastblog/datascience",
            "relUrl": "/datascience",
            "date": " • Feb 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "A basic ML workflow template",
            "content": "Brief Intro . Its good to simplify a problem and walk over it till the end. We choose the cleanest most agreeable data to walk through our data science processs. But first, we explain what machine learning is, and how supervised and unsupervised models differ. . Supervised and Unsupervised Learning . Why machine learning? Machine learning helps solve a unique class of problems. Take for example, facial recognition, or language translation. These problems come ever close to how humans perceive the world. Such that machines are now an embeded part of human interaction, without which, we feel less of ourselves...This is only the beginning. . Machine learning has 3 main flavours, Supervised, Unsupervised and Reinforcement Learning. The main data structures are tabular data, image data, language and timeseries. In supervised learning, we know what the ground truth is. We have recorded enough outcomes given certain events and interactions. The outcomes are the labels or dependent variables (y), that are the end product of feature interactions of independent variables(x). A model is a recipe of features that can map feature interaction to a label with certain degree of acceptance. The degree of acceptance is accuracy in a reproducable and generalised way. Simply put, a model is a function that maps x(s) to y. . In unsupervised learning, we dont have labels, we learn labels or categories from features. . More about simple supervised models. . Supervised models are either classification or regression type. Classification models predict the class labels from a finite group i.e boy or girl, species of plant, type of disease etc. Regression models predict labels as continous variables. i.e house price, fuel consumption etc. . The enterprise workflow . This workflow is explained futher here. . Step 0 Problem statement and libraries needed . We are given an array of petal and sepal measurements for 3 iris species. Our role is predict the species correclty if given new taxonomic measurements of petals and sepals of the same species of flowers. This is clearly a classification problem. . # the data is already in sklearn. from sklearn import datasets import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns plt.style.use(&#39;ggplot&#39;) . Step 1: Loading and preliminary data inspection . iris = datasets.load_iris() print(f&#39; type iris: {type(iris)}&#39;) print(f&#39;iris keys: {iris.keys()}&#39;) print(f&#39;type iris.data: {type(iris.data)}&#39;) print(f&#39;type iris.target: {type(iris.target)}&#39;) . type iris: &lt;class &#39;sklearn.utils.Bunch&#39;&gt; iris keys: dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;]) type iris.data: &lt;class &#39;numpy.ndarray&#39;&gt; type iris.target: &lt;class &#39;numpy.ndarray&#39;&gt; . . Note: this is a dictionary of numpy.array values. We will have to create out pandas dataframe using the keys. . df= pd.DataFrame(iris.data, columns= iris.feature_names) df.info() df.describe() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 150 entries, 0 to 149 Data columns (total 4 columns): # Column Non-Null Count Dtype -- -- 0 sepal length (cm) 150 non-null float64 1 sepal width (cm) 150 non-null float64 2 petal length (cm) 150 non-null float64 3 petal width (cm) 150 non-null float64 dtypes: float64(4) memory usage: 4.8 KB . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . count 150.000000 | 150.000000 | 150.000000 | 150.000000 | . mean 5.843333 | 3.057333 | 3.758000 | 1.199333 | . std 0.828066 | 0.435866 | 1.765298 | 0.762238 | . min 4.300000 | 2.000000 | 1.000000 | 0.100000 | . 25% 5.100000 | 2.800000 | 1.600000 | 0.300000 | . 50% 5.800000 | 3.000000 | 4.350000 | 1.300000 | . 75% 6.400000 | 3.300000 | 5.100000 | 1.800000 | . max 7.900000 | 4.400000 | 6.900000 | 2.500000 | . df.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . Step 2 Data Exploration and Visualization . We always want to visualize data. The reason is 2 fold. It will help you draw up conclusions fast and it will most likely be the method in which you communicate your findings. . sns.heatmap(df.corr()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f42f8a4a7d0&gt; . sns.pairplot(df) . &lt;seaborn.axisgrid.PairGrid at 0x7f42f8aa43d0&gt; . TODO: Add notes We aready know that our label had class of 3 flower . Step 3: Training the model . from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors = 6) x , y = iris.data, iris.target knn.fit(x, y) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=6, p=2, weights=&#39;uniform&#39;) . . Note: note we did not fit on a dataframe. We fit our model on nympy.array. . x.shape, y.shape, type(x), type(y) . ((150, 4), (150,), numpy.ndarray, numpy.ndarray) . Step 4 Making predictions on test data . new_data = np.array([[5.6, 2.8, 3.9, 1.1], [4.0, 2.1, 1.0, 0.2], [4.3, 3.6, 1.0, 0.3], [5.7, 2.6, 3.8, 1.3]]) prediction = knn.predict(new_data) print( prediction) . [1 0 0 1] . Step 5 Perfomance Metrics . What model would be compelete if did not try to measure how well it perfomed. Say, a scientist handed us new data, kneatly formated to suit our training data (how thoughtful), how convincing is pur classification model to accurately label the dataset? . Model accuracy. It means that your model is verifiable, generalizable and reproducable. . Our model, has no more data to test on, we used all our data to train. This is obviously a problem. We cant pose the same questions to our intelligent model, that we used to train it on. A 100% accuracy wouldnt be imperessive in this scenario. . So what should we have different? . Splitting our data into train, test and validation set. . For now, we stick to train and test. . from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score xtrain, xtest, ytrain, ytest = train_test_split(x, y , test_size = 0.2, random_state = 1, stratify = y) knn= KNeighborsClassifier(n_neighbors=8) knn.fit(xtrain, ytrain) ypred = knn.predict(xtest) print(ypred) . [2 0 1 0 0 0 2 2 2 1 0 1 2 1 2 0 2 1 1 2 1 1 0 0 2 1 0 0 1 1] . print(f&#39;score: {knn.score(xtest, ytest)}&#39;) print(f&#39;accuracy: {accuracy_score(ypred, ytest)}&#39;) . score: 0.9666666666666667 accuracy: 0.9666666666666667 . . Note: knn.score calls accuracy_score under the hood, that is why they give the same result. . We are at 97% model accuracy in telling apart iris flower species. . It be nice if we say the predicions as flower species names as opposed to numbers? Lets decode the predictions . from sklearn.preprocessing import LabelEncoder le = LabelEncoder().fit(iris.target_names) le.inverse_transform(ypred) . array([&#39;virginica&#39;, &#39;setosa&#39;, &#39;versicolor&#39;, &#39;setosa&#39;, &#39;setosa&#39;, &#39;setosa&#39;, &#39;virginica&#39;, &#39;virginica&#39;, &#39;virginica&#39;, &#39;versicolor&#39;, &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;, &#39;versicolor&#39;, &#39;virginica&#39;, &#39;setosa&#39;, &#39;virginica&#39;, &#39;versicolor&#39;, &#39;versicolor&#39;, &#39;virginica&#39;, &#39;versicolor&#39;, &#39;versicolor&#39;, &#39;setosa&#39;, &#39;setosa&#39;, &#39;virginica&#39;, &#39;versicolor&#39;, &#39;setosa&#39;, &#39;setosa&#39;, &#39;versicolor&#39;, &#39;versicolor&#39;], dtype=&#39;&lt;U10&#39;) . There is alot we didnt cover, eg our chosen KNN model intuition, what were other alternative models, would our model generalize well, is it reproducable etc. That comes later. . We now have a canvas to start with, not one we are proud of, but a canvas no matter. This is very weak canvas that barely paints the horizon, we have over simplified our responsibilites, on purpose. Later, we will revisit each step, go off on tangents, grow the number of steps and try more demanding challenges. . To learn more : .",
            "url": "https://maclawry.github.io/fastblog/mlworkflow",
            "relUrl": "/mlworkflow",
            "date": " • Jan 19, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This is my rendition of the IBM Analytics Data Science Competency Model through projects and notes. It is in no partcicular order and I will most likely not represent the core deliverables of the blue print, but I will try. . You can reach me on . TODO: add contact form .",
          "url": "https://maclawry.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Consult",
          "content": "TODO: add business ready projects here… .",
          "url": "https://maclawry.github.io/fastblog/consult/",
          "relUrl": "/consult/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://maclawry.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}